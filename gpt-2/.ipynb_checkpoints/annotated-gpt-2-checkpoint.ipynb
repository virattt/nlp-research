{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o-3eVawmBBay"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules import ModuleList\n",
    "from torch.nn.modules.normalization import LayerNorm\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm_notebook, trange\n",
    "import logging\n",
    "logging.basicConfig(level = logging.INFO)\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tkmlNWEfF0vj"
   },
   "source": [
    "### The Conv1D Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HVWbe3pjA-Tx"
   },
   "outputs": [],
   "source": [
    "class Conv1D(nn.Module):\n",
    "    def __init__(self, nx, nf):\n",
    "        super().__init__()\n",
    "        self.nf = nf\n",
    "        w = torch.empty(nx, nf)\n",
    "        nn.init.normal_(w, std=0.02)\n",
    "        self.weight = nn.Parameter(w)\n",
    "        self.bias = nn.Parameter(torch.zeros(nf))\n",
    "\n",
    "    def forward(self, x):\n",
    "        size_out = x.size()[:-1] + (self.nf,)\n",
    "        x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)\n",
    "        x = x.view(*size_out)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3416,
     "status": "ok",
     "timestamp": 1591218922059,
     "user": {
      "displayName": "Virat Singh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjudMqgPfvES2l3QRv20YuWSjknKe33-VUt6EKqX58=s64",
      "userId": "16468710768682078334"
     },
     "user_tz": 420
    },
    "id": "Div9jUxvESMn",
    "outputId": "5c93029e-e097-43ad-f1f1-251084e122bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 768])\n",
      "torch.Size([1, 4, 2304])\n"
     ]
    }
   ],
   "source": [
    "d_model = 768\n",
    "conv1d = Conv1D(d_model, d_model * 3)\n",
    "x = torch.rand(1, 4, d_model)\n",
    "print(x.shape)\n",
    "x = conv1d(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3410,
     "status": "ok",
     "timestamp": 1591218922059,
     "user": {
      "displayName": "Virat Singh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjudMqgPfvES2l3QRv20YuWSjknKe33-VUt6EKqX58=s64",
      "userId": "16468710768682078334"
     },
     "user_tz": 420
    },
    "id": "6AQ3qyZqEbb-",
    "outputId": "dbd80479-8f5a-4f91-d679-8d8b56207c5d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 4, 768]), torch.Size([1, 4, 768]), torch.Size([1, 4, 768]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query, key, value = x.split(d_model, dim=-1)\n",
    "\n",
    "query.shape, key.shape, value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3624,
     "status": "ok",
     "timestamp": 1591218922278,
     "user": {
      "displayName": "Virat Singh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjudMqgPfvES2l3QRv20YuWSjknKe33-VUt6EKqX58=s64",
      "userId": "16468710768682078334"
     },
     "user_tz": 420
    },
    "id": "6idma5wDE8S-",
    "outputId": "bb23ca6a-7d77-486b-831c-8cb4c6aae201"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1139,  0.3433, -0.7012,  ...,  0.2922, -0.4984,  0.5661],\n",
       "         [ 0.1224, -0.0440, -0.3904,  ...,  0.3896, -0.0977,  0.2255],\n",
       "         [-0.0081,  0.1536, -0.3442,  ...,  0.1474, -0.2547,  0.0604],\n",
       "         [ 0.1487,  0.0652, -0.4033,  ...,  0.2202, -0.2611,  0.2862]]],\n",
       "       grad_fn=<SplitBackward>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nfZUF-0eF5Jh"
   },
   "source": [
    "### The Feed Forward Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uliIp14BFo-e"
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dropout, d_model=768, nx=768*4):\n",
    "        super().__init__()\n",
    "        self.c_fc     = Conv1D(d_model, nx)\n",
    "        self.c_proj   = Conv1D(nx, d_model)\n",
    "        self.act      = F.gelu\n",
    "        self.dropout  = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        a     = self.act(self.c_fc(x))  # Apply acvitation on the first layer\n",
    "        proj  = self.c_proj(a)          # Project the previous layer back to original, 768 dimension\n",
    "\n",
    "        return self.dropout(proj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KpEQCcxlJYB9"
   },
   "source": [
    "### The Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-CMw2VeNGwaI"
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, d_model=768, n_head=12, n_ctx=1024, d_head=64, bias=True, scale=False):\n",
    "        super().__init__()\n",
    "        self.n_head  = n_head\n",
    "        self.d_model = d_model\n",
    "        self.c_attn  = Conv1D(d_model, d_model*3)\n",
    "        self.scale   = scale\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(n_ctx, n_ctx)).view(1, 1, n_ctx, n_ctx))\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.c_proj  = Conv1D(d_model, d_model)\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        \"return shape [`batch`, `head`, `sequence`, `features`]\"\n",
    "        new_shape = x.size()[:-1] + (self.n_head, x.size(-1)//self.n_head) \n",
    "        x = x.view(*new_shape)\n",
    "        return x.permute(0, 2, 1, 3) \n",
    "    \n",
    "    def _attn(self, q, k, v, attn_mask=None):\n",
    "        scores  = torch.matmul(q, k.transpose(-2, -1))\n",
    "        if self.scale: scores = scores/math.sqrt(v.size(-1))\n",
    "        nd, ns  = scores.size(-2), scores.size(-1)\n",
    "        if attn_mask is not None: scores = scores + attn_mask\n",
    "        scores  = self.softmax(scores)\n",
    "        scores  = self.dropout(scores)\n",
    "        outputs = torch.matmul(scores, v)\n",
    "        return outputs\n",
    "    \n",
    "    def merge_heads(self, x):\n",
    "        x         = x.permute(0, 2, 1, 3).contiguous()\n",
    "        new_shape = x.size()[:-2] + (x.size(-2)*x.size(-1),)\n",
    "        return x.view(*new_shape)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x        = self.c_attn(x) #new `x` shape - `[1,3,2304]`\n",
    "        q, k, v  = x.split(self.d_model, dim=2)\n",
    "        q, k, v  = self.split_heads(q), self.split_heads(k), self.split_heads(v)\n",
    "        out      = self._attn(q, k, v)\n",
    "        out      = self.merge_heads(out)\n",
    "        out      = self.c_proj(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xty_ggUfO0B0"
   },
   "source": [
    "### The Transformer-Decoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jtga3OZJN8BX"
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model=768, n_head=12, dropout=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attn         = Attention(d_model=d_model, n_head=n_head, n_ctx=1024, d_head=64,bias=True, scale=False)\n",
    "        self.feedforward  = FeedForward(dropout=0.1, d_model=d_model, nx=d_model * 4)\n",
    "        self.ln_1         = LayerNorm(d_model)\n",
    "        self.ln_2         = LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Attention block\n",
    "        normalized_1 = self.ln_1(x)\n",
    "        x = x + self.attn(normalized_1)\n",
    "\n",
    "        # Feed Forward block\n",
    "        normalized_2 = self.ln_2(x)\n",
    "        x = x + self.feedforward(normalized_2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The GPT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bq_I769yPgd_"
   },
   "outputs": [],
   "source": [
    "def _get_clones(module, n):\n",
    "    return ModuleList([copy.deepcopy(module) for i in range(n)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2(nn.Module):\n",
    "    def __init__(self, nlayers=12, n_ctx=1024, d_model=768, vcb_sz=50257):\n",
    "        super(GPT2, self).__init__()\n",
    "        self.nlayers = nlayers\n",
    "        block        = TransformerBlock(d_model=768, n_head=12, dropout=0.1)\n",
    "        self.h  = _get_clones(block, 12)\n",
    "        self.wte     = nn.Embedding(vcb_sz, d_model)\n",
    "        self.wpe     = nn.Embedding(n_ctx, d_model)\n",
    "        self.drop    = nn.Dropout(0.1)\n",
    "        self.ln_f    = LayerNorm(d_model)\n",
    "        self.out     = nn.Linear(d_model, vcb_sz, bias=False)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        self.out.weight = self.wte.weight\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding, Conv1D)):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if isinstance(module, (nn.Linear, Conv1D)) and module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "            \n",
    "    \n",
    "    def forward(self, src, labels=None, pos_ids=None):\n",
    "        if pos_ids is None: pos_ids = torch.arange(0, src.size(-1)).unsqueeze(0)\n",
    "        inp = self.drop((self.wte(src) + self.wpe(pos_ids)))\n",
    "        for i in range(self.nlayers): inp = self.h[i](inp)\n",
    "            \n",
    "        inp     = self.ln_f(inp) \n",
    "        logits  = self.out(inp)        # dim is \n",
    "        outputs = (logits,) + (inp,)   # dim is\n",
    "        \n",
    "        # For classification (?)\n",
    "        if labels is not None:\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            loss = self.loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "            outputs = (loss,) + outputs\n",
    "            return outputs\n",
    "        return logits\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Hugging Face Pretrained Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z_txTkWHCem0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2(\n",
       "  (t): ModuleList(\n",
       "    (0): TransformerBlock(\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (softmax): Softmax(dim=-1)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (c_proj): Conv1D()\n",
       "      )\n",
       "      (feedforward): FeedForward(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (softmax): Softmax(dim=-1)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (c_proj): Conv1D()\n",
       "      )\n",
       "      (feedforward): FeedForward(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (softmax): Softmax(dim=-1)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (c_proj): Conv1D()\n",
       "      )\n",
       "      (feedforward): FeedForward(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (softmax): Softmax(dim=-1)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (c_proj): Conv1D()\n",
       "      )\n",
       "      (feedforward): FeedForward(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (softmax): Softmax(dim=-1)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (c_proj): Conv1D()\n",
       "      )\n",
       "      (feedforward): FeedForward(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (softmax): Softmax(dim=-1)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (c_proj): Conv1D()\n",
       "      )\n",
       "      (feedforward): FeedForward(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (softmax): Softmax(dim=-1)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (c_proj): Conv1D()\n",
       "      )\n",
       "      (feedforward): FeedForward(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (softmax): Softmax(dim=-1)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (c_proj): Conv1D()\n",
       "      )\n",
       "      (feedforward): FeedForward(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (softmax): Softmax(dim=-1)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (c_proj): Conv1D()\n",
       "      )\n",
       "      (feedforward): FeedForward(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (softmax): Softmax(dim=-1)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (c_proj): Conv1D()\n",
       "      )\n",
       "      (feedforward): FeedForward(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (softmax): Softmax(dim=-1)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (c_proj): Conv1D()\n",
       "      )\n",
       "      (feedforward): FeedForward(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (softmax): Softmax(dim=-1)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (c_proj): Conv1D()\n",
       "      )\n",
       "      (feedforward): FeedForward(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (wte): Embedding(50257, 768)\n",
       "  (wpe): Embedding(1024, 768)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (out): Linear(in_features=768, out_features=50257, bias=False)\n",
       "  (loss_fn): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT2()\n",
    "# load pretrained_weights from hugging face\n",
    "# download file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin to `.`\n",
    "\n",
    "model_dict = model.state_dict() #currently with random initialization\n",
    "state_dict = torch.load(\"./gpt2-pytorch_model.bin\") #pretrained weights\n",
    "\n",
    "old_keys = []\n",
    "new_keys = []\n",
    "for key in state_dict.keys(): \n",
    "    if \"mlp\" in key: #The hugging face state dict references the feedforward network as mlp, need to replace to `feedforward` be able to reuse these weights\n",
    "        new_key = key.replace(\"mlp\", \"feedforward\")\n",
    "        new_keys.append(new_key)\n",
    "        old_keys.append(key)\n",
    "\n",
    "for old_key, new_key in zip(old_keys, new_keys): \n",
    "    state_dict[new_key]=state_dict.pop(old_key)\n",
    "\n",
    "pretrained_dict = {k: v for k, v in state_dict.items() if k in model_dict}\n",
    "\n",
    "model_dict.update(pretrained_dict)\n",
    "model.load_state_dict(model_dict)\n",
    "model.eval() #model in inference mode as it's now initialized with pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['wte.weight', 'wpe.weight', 'ln_f.weight', 'ln_f.bias'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Text Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at C:\\Users\\virat/.cache\\torch\\transformers\\f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
      "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at C:\\Users\\virat/.cache\\torch\\transformers\\d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The Planet Earthelectelectelectelectelect guiActiveUnfocused guiActiveUnfocused guiActiveUnfocused guiActiveUnfocused guiActiveUnfocused guiActiveUnfocused guiActiveUnfocused guiActiveUnfocused guiActiveUnfocused guiActiveUnfocused guiActiveUnfocused guiActiveUnfocused guiActiveUnfocused guiActiveUnfocused guiActiveUnfocused'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "context = torch.tensor([tokenizer.encode(\"The Planet Earth\")])\n",
    "\n",
    "def generate(context, n_tokens=20):\n",
    "    for _ in range(n_tokens):\n",
    "        out = model(context)\n",
    "        logits = out[:, -1, :]\n",
    "        indices_to_remove = logits < torch.topk(logits, 10)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = np.NINF\n",
    "        next_token = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1).squeeze(1)\n",
    "        \n",
    "        # Append the new `next_token` to the context\n",
    "        context = torch.cat([context, next_token.unsqueeze(-1)], dim=-1)\n",
    "    return context\n",
    "\n",
    "out = generate(context, n_tokens=20)\n",
    "tokenizer.decode(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0,  1,  2],\n",
       "        [ 2,  3,  4],\n",
       "        [ 4,  5,  6]],\n",
       "\n",
       "       [[ 8,  9, 10],\n",
       "        [10, 11, 12],\n",
       "        [12, 13, 14]],\n",
       "\n",
       "       [[16, 17, 18],\n",
       "        [18, 19, 20],\n",
       "        [20, 21, 22]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = np.array([[[i + 2*j + 8*k for i in range(3)] for j in range(3)] for k in range(3)])\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8,  9, 10],\n",
       "       [10, 11, 12],\n",
       "       [12, 13, 14]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[1,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  3,  5],\n",
       "       [ 9, 11, 13],\n",
       "       [17, 19, 21]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[..., 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOsksarDTxJzPGlNFBAkgn6",
   "collapsed_sections": [
    "tkmlNWEfF0vj",
    "nfZUF-0eF5Jh"
   ],
   "name": "annotated-gpt-2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
